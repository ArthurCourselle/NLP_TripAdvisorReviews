{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = \n",
    "# tokens = [nltk.word_tokenize(sentence.lower()) for sentence in train_list]\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(train_df['User Prompt'])\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(data['userprompt'])\n",
    "\n",
    "# y = train_df['Prompt injection']\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def loss(y_pred, y):\n",
    "    return -(1/len(y_pred)) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - (y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the chain rule to get the gradient.\n",
    "def grad(y_pred, y, X):\n",
    "    # return (1/len(X)) * np.sum(((w * X + b) - y)_)\n",
    "    Jw = (1/len(y_pred)) * X.T @ (y_pred - y)\n",
    "    Jb = ()\n",
    "    # Jw = (y_pred - y)\n",
    "\n",
    "def predict(X, w, b):\n",
    "    # Softmax ?\n",
    "    \n",
    "\n",
    "w = np.random.randn(train_df.shape[1], 2)\n",
    "b = np.zeros((1,2))\n",
    "w\n",
    "\n",
    "lr = 0.001\n",
    "num_iter = 100\n",
    "for i in range(num_iter):\n",
    "    y_pred = sigmoid(w @ X + b)\n",
    "    Jw, Jb = grad(train_df, train_df[])\n",
    "    w -= lr * Jw\n",
    "    b -= lr * Jb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ing2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
